{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Iterative Classifier.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "79454f6df3b649da8b1ab37b9e2f4024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ccc8aa7b15f5476fb30ca6eeeb5a5e03",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_25cf1598404f4036a542727df0129302",
              "IPY_MODEL_78d8accfe0fc4da091990139d622466b"
            ]
          }
        },
        "ccc8aa7b15f5476fb30ca6eeeb5a5e03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "25cf1598404f4036a542727df0129302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_70708c4e18974ac1adb18bf3b2081d37",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 286561,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 286561,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5d84a9dfaa394f4f992a733590b3eb83"
          }
        },
        "78d8accfe0fc4da091990139d622466b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fab8ea8bc38d46fe807f21e2f4424f87",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 286561/286561 [00:16&lt;00:00, 17206.69it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23d822ac13544f9ca8f5ba8a86b6aa24"
          }
        },
        "70708c4e18974ac1adb18bf3b2081d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5d84a9dfaa394f4f992a733590b3eb83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fab8ea8bc38d46fe807f21e2f4424f87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "23d822ac13544f9ca8f5ba8a86b6aa24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0JCwszo1to2"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pylab\n",
        "import pickle\n",
        "import numpy as np\n",
        "import time\n",
        "import itertools\n",
        "from tqdm.notebook import tqdm\n",
        "from multiprocessing import Pool\n",
        "import matplotlib.cm as cm\n",
        "import scipy\n",
        "import community as community_louvain\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.sparse as sp\n",
        "from sklearn.metrics import accuracy_score,classification_report\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEYc7G6-1to9"
      },
      "source": [
        "# Class Definitions\n",
        "adapted from https://github.com/tkipf/ica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xJyRZa-1to-"
      },
      "source": [
        "def get_class(kls):\n",
        "    parts = kls.split('.')\n",
        "    module = \".\".join(parts[:-1])\n",
        "    md = __import__(module)\n",
        "    for comp in parts[1:]:\n",
        "        md = getattr(md, comp)\n",
        "    return md\n",
        "\n",
        "\n",
        "def build_graph(adj, features, labels):\n",
        "    edges = np.array(adj.nonzero()).T\n",
        "    y_values = np.array(labels.nonzero()).T\n",
        "\n",
        "    domain_labels = []\n",
        "    for i in range(labels.shape[1]):\n",
        "        domain_labels.append(\"c\" + str(i))\n",
        "\n",
        "    # create graph\n",
        "    graph = UndirectedGraph()\n",
        "    id_obj_map = []\n",
        "    for i in range(adj.shape[0]):\n",
        "        n = Node(i, features[i, :], domain_labels[y_values[i, 1]])\n",
        "        graph.add_node(n)\n",
        "        id_obj_map.append(n)\n",
        "    for e in edges:\n",
        "        graph.add_edge(Edge(id_obj_map[e[1]], id_obj_map[e[0]]))\n",
        "\n",
        "    return graph, domain_labels\n",
        "\n",
        "\n",
        "def create_map(graph, train_indices):\n",
        "    conditional_map = {}\n",
        "    for i in train_indices:\n",
        "        conditional_map[graph.node_list[i]] = graph.node_list[i].label\n",
        "    return conditional_map\n",
        "\n",
        "\n",
        "\n",
        "class Classifier(object):\n",
        "    def __init__(self, scikit_classifier_name, **classifier_args):\n",
        "        classifer_class = get_class(scikit_classifier_name)\n",
        "        self.clf = classifer_class(**classifier_args)\n",
        "\n",
        "    def fit(self, graph, train_indices):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def predict(self, graph, test_indices, conditional_node_to_label_map=None):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class LocalClassifier(Classifier):\n",
        "    def fit(self, graph, train_indices):\n",
        "\n",
        "        feature_list = []\n",
        "        label_list = []\n",
        "        g = graph\n",
        "        n = g.node_list\n",
        "        training_nodes = [n[i] for i in train_indices]\n",
        "\n",
        "        for nodes in training_nodes:\n",
        "            feature_list.append(nodes.feature_vector)\n",
        "            label_list.append(nodes.label)\n",
        "\n",
        "        feature_list = sp.vstack(feature_list)\n",
        "        feature_list = sp.csr_matrix(feature_list, dtype=np.float64)\n",
        "\n",
        "        self.clf.fit(feature_list, label_list)\n",
        "        return\n",
        "\n",
        "    def predict(self, graph, test_indices, conditional_node_to_label_map=None):\n",
        "\n",
        "        feature_list = []\n",
        "        g = graph\n",
        "        n = g.node_list\n",
        "        testing_nodes = [n[i] for i in test_indices]\n",
        "\n",
        "        for nodes in testing_nodes:\n",
        "            feature_list.append(nodes.feature_vector)\n",
        "\n",
        "        feature_list = sp.vstack(feature_list)\n",
        "        feature_list = sp.csr_matrix(feature_list, dtype=np.float64)\n",
        "\n",
        "        y = self.clf.predict(feature_list)\n",
        "        return y\n",
        "\n",
        "\n",
        "class RelationalClassifier(Classifier):\n",
        "    def __init__(self, scikit_classifier_name, aggregator, **classifier_args):\n",
        "        super(RelationalClassifier, self).__init__(scikit_classifier_name, **classifier_args)\n",
        "        self.aggregator = aggregator\n",
        "\n",
        "    def fit(self, graph, train_indices, local_classifier, bootstrap):\n",
        "        conditional_map = {}\n",
        "\n",
        "        if bootstrap:\n",
        "            predictclf = local_classifier.predict(graph, range(len(graph.node_list)))\n",
        "            conditional_map = self.cond_mp_upd(graph, conditional_map, predictclf, range(len(graph.node_list)))\n",
        "\n",
        "        for i in train_indices:\n",
        "            conditional_map[graph.node_list[i]] = graph.node_list[i].label\n",
        "        features = []\n",
        "        aggregates = []\n",
        "        labels = []\n",
        "        for i in train_indices:\n",
        "            features.append(graph.node_list[i].feature_vector)\n",
        "            labels.append(graph.node_list[i].label)\n",
        "            aggregates.append(sp.csr_matrix(self.aggregator.aggregate(graph,\n",
        "                                                                      graph.node_list[i],\n",
        "                                                                      conditional_map), dtype=np.float64))\n",
        "        features = sp.vstack(features)\n",
        "        features = sp.csr_matrix(features, dtype=np.float64)\n",
        "        aggregates = sp.vstack(aggregates)\n",
        "        features = sp.hstack([features, aggregates])\n",
        "\n",
        "        self.clf.fit(features, labels)\n",
        "\n",
        "    def predict(self, graph, test_indices, conditional_map=None):\n",
        "        features = []\n",
        "        aggregates = []\n",
        "\n",
        "        for i in test_indices:\n",
        "            features.append(graph.node_list[i].feature_vector)\n",
        "            aggregates.append(sp.csr_matrix(self.aggregator.aggregate(graph,\n",
        "                                                                      graph.node_list[i],\n",
        "                                                                      conditional_map), dtype=np.float64))\n",
        "        features = sp.vstack(features)\n",
        "        features = sp.csr_matrix(features, dtype=np.float64)\n",
        "        aggregates = sp.vstack(aggregates)\n",
        "        features = sp.hstack([features, aggregates])\n",
        "\n",
        "        return self.clf.predict(features)\n",
        "\n",
        "    def cond_mp_upd(self, graph, conditional_map, pred, indices):\n",
        "        for x in range(len(pred)):\n",
        "            conditional_map[graph.node_list[indices[x]]] = pred[x]\n",
        "        return conditional_map\n",
        "\n",
        "class Graph(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Create an empty graph\n",
        "        '''\n",
        "        self.node_list = []\n",
        "        self.edge_list = []        \n",
        "    \n",
        "    def add_node(self, n):\n",
        "        self.node_list.append(n)        \n",
        "    \n",
        "    def add_edge(self, e):\n",
        "        abstract()\n",
        "    \n",
        "    def get_neighbors(self, n):\n",
        "        abstract()\n",
        "\n",
        "class Node(object):\n",
        "    def __init__(self, node_id, feature_vector = None, label = None):\n",
        "        self.node_id = node_id\n",
        "        self.feature_vector = feature_vector\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class Edge(object):\n",
        "    def __init__(self, from_node, to_node, feature_vector = None, label = None):\n",
        "        self.from_node = from_node\n",
        "        self.to_node = to_node\n",
        "        self.feature_vector = feature_vector\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class DirectedGraph(Graph):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(DirectedGraph, self).__init__()\n",
        "        self.out_neighbors = defaultdict(set)\n",
        "        self.in_neighbors = defaultdict(set)\n",
        "        self.str_class=[]\n",
        "\n",
        "    def add_edge(self, e):\n",
        "        self.edge_list.append(e)\n",
        "        self.out_neighbors[e.from_node].add(e.to_node)\n",
        "        self.in_neighbors[e.to_node].add(e.from_node)\n",
        "    \n",
        "    def get_out_neighbors(self, n):\n",
        "        return self.out_neighbors[n]\n",
        "    \n",
        "    def get_in_neighbors(self, n):\n",
        "        return self.in_neighbors[n]\n",
        "    \n",
        "    def get_neighbors(self, n):\n",
        "        return self.out_neighbors[n].union(self.in_neighbors[n])\n",
        "\n",
        "class UndirectedGraph(Graph):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(UndirectedGraph, self).__init__()\n",
        "        self.neighbors = defaultdict(set)        \n",
        "    \n",
        "    def add_edge(self, e):\n",
        "        self.neighbors[e.from_node].add(e.to_node)\n",
        "        self.neighbors[e.to_node].add(e.from_node)\n",
        "    \n",
        "    def get_neighbors(self, n):\n",
        "        return self.neighbors[n]\n",
        "\n",
        "        \n",
        "        super(RelationalClassifier, self).__init__(scikit_classifier_name, **classifier_args)\n",
        "        self.aggregator = aggregator\n",
        "\n",
        "    def fit(self, graph, train_indices, local_classifier, bootstrap):\n",
        "        conditional_map = {}\n",
        "\n",
        "        if bootstrap:\n",
        "            predictclf = local_classifier.predict(graph, range(len(graph.node_list)))\n",
        "            conditional_map = self.cond_mp_upd(graph, conditional_map, predictclf, range(len(graph.node_list)))\n",
        "\n",
        "        for i in train_indices:\n",
        "            conditional_map[graph.node_list[i]] = graph.node_list[i].label\n",
        "        features = []\n",
        "        aggregates = []\n",
        "        labels = []\n",
        "        for i in train_indices:\n",
        "            features.append(graph.node_list[i].feature_vector)\n",
        "            labels.append(graph.node_list[i].label)\n",
        "            aggregates.append(sp.csr_matrix(self.aggregator.aggregate(graph,\n",
        "                                                                      graph.node_list[i],\n",
        "                                                                      conditional_map), dtype=np.float64))\n",
        "        features = sp.vstack(features)\n",
        "        features = sp.csr_matrix(features, dtype=np.float64)\n",
        "        aggregates = sp.vstack(aggregates)\n",
        "        features = sp.hstack([features, aggregates])\n",
        "\n",
        "        self.clf.fit(features, labels)\n",
        "\n",
        "    def predict(self, graph, test_indices, conditional_map=None):\n",
        "        features = []\n",
        "        aggregates = []\n",
        "\n",
        "        for i in test_indices:\n",
        "            features.append(graph.node_list[i].feature_vector)\n",
        "            aggregates.append(sp.csr_matrix(self.aggregator.aggregate(graph,\n",
        "                                                                      graph.node_list[i],\n",
        "                                                                      conditional_map), dtype=np.float64))\n",
        "        features = sp.vstack(features)\n",
        "        features = sp.csr_matrix(features, dtype=np.float64)\n",
        "        aggregates = sp.vstack(aggregates)\n",
        "        features = sp.hstack([features, aggregates])\n",
        "\n",
        "        return self.clf.predict(features)\n",
        "\n",
        "    def cond_mp_upd(self, graph, conditional_map, pred, indices):\n",
        "        for x in range(len(pred)):\n",
        "            conditional_map[graph.node_list[indices[x]]] = pred[x]\n",
        "        return conditional_map\n",
        "\n",
        "\n",
        "class ICA(Classifier):\n",
        "    def __init__(self, local_classifier, relational_classifier, bootstrap, max_iteration=10):\n",
        "        self.local_classifier = local_classifier\n",
        "        self.relational_classifier = relational_classifier\n",
        "        self.bootstrap = bootstrap\n",
        "        self.max_iteration = max_iteration\n",
        "\n",
        "    def fit(self, graph, train_indices):\n",
        "        self.local_classifier.fit(graph, train_indices)\n",
        "        self.relational_classifier.fit(graph, train_indices, self.local_classifier, self.bootstrap)\n",
        "\n",
        "    def predict(self, graph, eval_indices, test_indices, conditional_node_to_label_map=None):\n",
        "        predictclf = self.local_classifier.predict(graph, eval_indices)\n",
        "        conditional_node_to_label_map = self.cond_mp_upd(graph,\n",
        "                                                         conditional_node_to_label_map,\n",
        "                                                         predictclf, eval_indices)\n",
        "\n",
        "        relation_predict = []\n",
        "        temp = []\n",
        "        for iter in range(self.max_iteration):\n",
        "            for x in eval_indices:\n",
        "                temp.append(x)\n",
        "                rltn_pred = list(self.relational_classifier.predict(graph, temp, conditional_node_to_label_map))\n",
        "                conditional_node_to_label_map = self.cond_mp_upd(graph, conditional_node_to_label_map, rltn_pred, temp)\n",
        "                temp.remove(x)\n",
        "        for ti in test_indices:\n",
        "            relation_predict.append(conditional_node_to_label_map[graph.node_list[ti]])\n",
        "        return relation_predict\n",
        "    def cond_mp_upd(self, graph, conditional_map, pred, indices):\n",
        "        for x in range(len(pred)):\n",
        "            conditional_map[graph.node_list[indices[x]]] = pred[x]\n",
        "        return conditional_map\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-3A4PPV1tpH"
      },
      "source": [
        "class Aggregator(object):\n",
        "    def __init__(self, domain_labels):\n",
        "        self.domain_labels = domain_labels  # The list of labels in the domain\n",
        "\n",
        "    def aggregate(self, graph, node, conditional_node_to_label_map):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Count(Aggregator):\n",
        "    def aggregate(self, graph, node, conditional_node_to_label_map):\n",
        "        neighbor_undirected = []\n",
        "\n",
        "        for x in self.domain_labels:\n",
        "            neighbor_undirected.append(0.0)\n",
        "        for i in graph.get_neighbors(node):\n",
        "            if i in conditional_node_to_label_map.keys():\n",
        "                index = self.domain_labels.index(conditional_node_to_label_map[i])\n",
        "                neighbor_undirected[index] += 1.0\n",
        "        return neighbor_undirected\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soN4GTIc1tpH"
      },
      "source": [
        "## Graph building\n",
        "\n",
        "Since we want to use an algortihm for node classification the processing needs to done a bit differently than originally intended. Most of the meta data in attriubte `PROPERTIES` relates to a post, but in our case posts are not the nodes but the edge between nodes (subreddits). This requires us to aggregate the sentiment from all Posts of a subreddit to a singe node classification by averaging `PROPERTIES` and `LINK_SENTIMENT`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAgJ_r4h33LT",
        "outputId": "c5d3ce20-314b-429f-ab46-2e72b1390ec7"
      },
      "source": [
        "!wget http://snap.stanford.edu/data/soc-redditHyperlinks-body.tsv"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-09 14:10:26--  http://snap.stanford.edu/data/soc-redditHyperlinks-body.tsv\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 318931394 (304M) [text/tab-separated-values]\n",
            "Saving to: â€˜soc-redditHyperlinks-body.tsvâ€™\n",
            "\n",
            "soc-redditHyperlink 100%[===================>] 304.16M  62.3MB/s    in 5.2s    \n",
            "\n",
            "2020-12-09 14:10:31 (58.4 MB/s) - â€˜soc-redditHyperlinks-body.tsvâ€™ saved [318931394/318931394]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIhGCFtV1tpH"
      },
      "source": [
        "df = pd.read_csv('soc-redditHyperlinks-body.tsv',sep='\\t')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mgemoph1tpI"
      },
      "source": [
        "G = nx.Graph()\n",
        "G.add_nodes_from(df['SOURCE_SUBREDDIT'].unique().tolist())\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtUWg1hm1tpI"
      },
      "source": [
        "The meta attributes of a post need to be formatted differently and are then added to the original DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "79454f6df3b649da8b1ab37b9e2f4024",
            "ccc8aa7b15f5476fb30ca6eeeb5a5e03",
            "25cf1598404f4036a542727df0129302",
            "78d8accfe0fc4da091990139d622466b",
            "70708c4e18974ac1adb18bf3b2081d37",
            "5d84a9dfaa394f4f992a733590b3eb83",
            "fab8ea8bc38d46fe807f21e2f4424f87",
            "23d822ac13544f9ca8f5ba8a86b6aa24"
          ]
        },
        "id": "74djEUn71tpI",
        "outputId": "452a4e39-1c9e-453f-94b8-6829983eaaee"
      },
      "source": [
        "props = df.pop('PROPERTIES')\n",
        "vals = props.str.split(',').values\n",
        "data = list()\n",
        "for val in tqdm(vals):\n",
        "    data.append(list(map(float,val)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79454f6df3b649da8b1ab37b9e2f4024",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=286561.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Wt33bP1tpJ"
      },
      "source": [
        "data_df = pd.DataFrame(data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCGbLfR81tpJ"
      },
      "source": [
        "full_df_props = pd.concat((df,data_df),axis=1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuTDz_911tpJ"
      },
      "source": [
        "Some subreddits never mention another subreddit, to make the aggregation simple we restrict the graph to contain only edges from posts that ever linked to another subreddit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsKn8fEG1tpK"
      },
      "source": [
        "mask = full_df_props['TARGET_SUBREDDIT'].isin(full_df_props['SOURCE_SUBREDDIT'])\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiXj4mao1tpK"
      },
      "source": [
        "Since a single subreddit can mention the same target subreddit mutliple times we need to aggregate the sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e40dTlDV1tpK"
      },
      "source": [
        "weights = full_df_props[mask].groupby(['SOURCE_SUBREDDIT','TARGET_SUBREDDIT'])['LINK_SENTIMENT'].mean().reset_index().values\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie-wODDG1tpK"
      },
      "source": [
        "Build up new Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I3np5Tj1tpK"
      },
      "source": [
        "G.add_weighted_edges_from(weights)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BPk_20Z1tpL",
        "outputId": "5c264e0f-ca7b-4b95-a8b2-d1ff67f3ca8f"
      },
      "source": [
        "comps = list(nx.connected_components(G))\n",
        "connected_subgrpah = G.subgraph(comps[0])\n",
        "nx.is_connected(connected_subgrpah)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilTXWb_W1tpL"
      },
      "source": [
        "Since some nodes are not connected we use the biggest network (accounting for the majority of nodes) and filter our dataset further so that only features of nodes in a complete subgraph are included"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J2QfxO31tpL"
      },
      "source": [
        "mask_connected_comp = full_df_props['SOURCE_SUBREDDIT'].isin(list(connected_subgrpah.nodes))\n",
        "feats_grouped_source = full_df_props[mask_connected_comp].groupby('SOURCE_SUBREDDIT')[data_df.columns].mean()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GkYDVTz1tpL"
      },
      "source": [
        "Create the labels for the individual nodes by aggregating all outwards edges to a mean sentiment score of the node. An intuition would maybe be how toxic a certain subreddit is. Afterwards to make this to a classification problem we group the Sentiment into two categories. The negative response (avg Sentiment between -1 and 0.9 and purely 1). We have to group the negative sentiment quite broad due to the enormous class divide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX9uW6k61tpM"
      },
      "source": [
        "labels = pd.get_dummies(\n",
        "    pd.cut(full_df_props[mask_connected_comp].groupby('SOURCE_SUBREDDIT')[\"LINK_SENTIMENT\"].mean(),[-1.1,0.9,1])).values\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbYiKSi81tpM"
      },
      "source": [
        "Create attributes required by original code from github link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anpSfScw1tpM"
      },
      "source": [
        "feats = feats_grouped_source.values\n",
        "features = scipy.sparse.csr_matrix(feats).tolil()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZnZBvIG1tpN"
      },
      "source": [
        "adj = nx.adjacency_matrix(connected_subgrpah)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCMZYxCg1tpO"
      },
      "source": [
        "graph, domain_labels = build_graph(adj, features, labels)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5T9aRBO1tpO"
      },
      "source": [
        "all_idx = list(range(len(feats_grouped_source)))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdwGif0A1tpO"
      },
      "source": [
        "Create train,test and validation split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvFFq4of1tpP"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(all_idx, labels, test_size=0.2, random_state=1,stratify=labels)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=1, stratify=y_train)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTn48BmI1tpP"
      },
      "source": [
        "idx_train = X_train\n",
        "\n",
        "idx_val = X_val\n",
        "\n",
        "idx_test = X_test"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK1wu6IQ1tpP"
      },
      "source": [
        "eval_idx = np.setdiff1d(range(adj.shape[0]), idx_train)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTYhuLVy1tpP"
      },
      "source": [
        "Do classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcUJe-An1tpQ",
        "outputId": "fa061ef7-dbbe-43ab-b964-7c1ba2391ade"
      },
      "source": [
        "for ep in range(1):\n",
        "    print('Iteration: ',ep)\n",
        "    np.random.shuffle(eval_idx)\n",
        "    y_true = [graph.node_list[t].label for t in idx_test]\n",
        "    local_clf = LocalClassifier('sklearn.linear_model.LogisticRegression')\n",
        "    agg = Count(domain_labels)\n",
        "    relational_clf = RelationalClassifier('sklearn.linear_model.LogisticRegression', agg)\n",
        "    ica = ICA(local_clf, relational_clf, True, max_iteration=10)\n",
        "    print('Training....')\n",
        "    ica.fit(graph, idx_train)\n",
        "    print('Training done!')\n",
        "    conditional_node_to_label_map = create_map(graph, idx_train)\n",
        "    print('Prediction...')\n",
        "    ica_predict = ica.predict(graph, eval_idx, idx_test, conditional_node_to_label_map)\n",
        "    print('Prediction Done!')\n",
        "    print(classification_report(ica_predict,y_true))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:  0\n",
            "Training....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training done!\n",
            "Prediction...\n",
            "Prediction Done!\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          c0       0.00      0.00      0.00         1\n",
            "          c1       1.00      0.87      0.93      5298\n",
            "\n",
            "    accuracy                           0.87      5299\n",
            "   macro avg       0.50      0.44      0.47      5299\n",
            "weighted avg       1.00      0.87      0.93      5299\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW0lmgrQ1tpQ"
      },
      "source": [
        "# ToDo\n",
        "\n",
        "- Currently accuracy is mainly driven by majority class, need to either undersample majority or oversample minority as a probable solution. Possible weighting inverse to occurences might help as well but need to be tested\n",
        "\n",
        "- Overall performance of IC is abysmal need to parallelize almost everything. For some reason prediction seems to be way slower than training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAfTzmZ11tpV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}